# Qalb Technical Architecture

## ğŸ—ï¸ Model Architecture

### Base Foundation

Qalb is built upon **Meta LLaMA 3.1 8B** as the foundation model, leveraging:

- **Architecture Type**: Transformer-based autoregressive language model
- **Total Parameters**: 8 Billion
- **Trainable Parameters**: ~1.18B (~14.72% of base via LoRA)
- **Precision**: bfloat16

### LoRA Configuration

Qalb uses **Low-Rank Adaptation (LoRA)** for efficient training:

| Parameter | Value |
|-----------|-------|
| LoRA Rank (r) | 128 |
| LoRA Alpha | 32 |
| Trainable Parameters | ~1.18B |
| Percentage of Base | ~14.72% |

### Model Specifications

| Specification | Details |
|--------------|---------|
| **Model Name** | Qalb-1.0-8B-Instruct |
| **Base Model** | unsloth/Meta-Llama-3.1-8B |
| **Model Size** | 8B parameters |
| **Tensor Type** | BF16 |
| **File Format** | Safetensors |
| **Sequence Length** | 2048 tokens |
| **Languages** | Urdu (primary), English (secondary) |

## ğŸ”§ Training Infrastructure

### Hardware

- **GPU**: Single NVIDIA A100 80GB
- **Framework**: Unsloth library
- **Features**: Memory-efficient attention mechanisms + fast LoRA implementations

### Training Configuration

| Parameter | Continued Pre-training | Fine-tuning |
|-----------|----------------------|-------------|
| **Optimizer** | AdamW-8bit | AdamW-8bit |
| **Learning Rate** | Cosine schedule (warmup 0.05) | 5e-5 (linear) |
| **Batch Size** | 128 (effective) | 64 |
| **Epochs** | - | 2 |
| **Precision** | bfloat16 | bfloat16 |
| **Gradient Checkpointing** | Yes | Yes |

## ğŸ“Š Two-Stage Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         QALB TRAINING PIPELINE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    STAGE 1: DATA CURATION                    â”‚    â”‚
â”‚  â”‚                                                               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚    â”‚
â”‚  â”‚  â”‚   Urdu Corpus    â”‚    â”‚  English Corpus  â”‚               â”‚    â”‚
â”‚  â”‚  â”‚  1.84B Tokens    â”‚    â”‚   140M Tokens    â”‚               â”‚    â”‚
â”‚  â”‚  â”‚                  â”‚    â”‚                  â”‚               â”‚    â”‚
â”‚  â”‚  â”‚  â€¢ News & Media  â”‚    â”‚  â€¢ Wikipedia     â”‚               â”‚    â”‚
â”‚  â”‚  â”‚  â€¢ Literature    â”‚    â”‚  (Replay Buffer) â”‚               â”‚    â”‚
â”‚  â”‚  â”‚  â€¢ Gov Documents â”‚    â”‚                  â”‚               â”‚    â”‚
â”‚  â”‚  â”‚  â€¢ Social Media  â”‚    â”‚                  â”‚               â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚    â”‚
â”‚  â”‚                     â†“                                         â”‚    â”‚
â”‚  â”‚              Data Cleaning Pipeline                           â”‚    â”‚
â”‚  â”‚         (67.8% retention rate)                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              STAGE 2: CONTINUED PRE-TRAINING                 â”‚    â”‚
â”‚  â”‚                                                               â”‚    â”‚
â”‚  â”‚  Base: unsloth/Meta-Llama-3.1-8B                            â”‚    â”‚
â”‚  â”‚  Method: LoRA (rank=128, alpha=32)                          â”‚    â”‚
â”‚  â”‚  Hardware: NVIDIA A100 80GB                                  â”‚    â”‚
â”‚  â”‚  Framework: Unsloth                                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚            STAGE 3: INSTRUCTION FINE-TUNING                  â”‚    â”‚
â”‚  â”‚                                                               â”‚    â”‚
â”‚  â”‚  Dataset: Alif Urdu-instruct                                 â”‚    â”‚
â”‚  â”‚  Format: Llama-3 chat template                               â”‚    â”‚
â”‚  â”‚  System Prompt: Urdu-speaking assistant                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    QALB-1.0-8B-INSTRUCT                      â”‚    â”‚
â”‚  â”‚                                                               â”‚    â”‚
â”‚  â”‚                   Overall Score: 90.34                        â”‚    â”‚
â”‚  â”‚                   Urdu Purity: 95.31%                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Prompt Format

Qalb uses the official **Llama-3 chat template** with distinct control tokens:

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

### Default System Prompt (Urdu)

```
Ø¢Ù¾ Ø§ÛŒÚ© Ù…Ø¯Ø¯Ú¯Ø§Ø± Ø§ÙˆØ± Ø¨Û’ Ø¶Ø±Ø± Ù…ØµÙ†ÙˆØ¹ÛŒ Ø°ÛØ§Ù†Øª Ú©Û’ Ø§Ø³Ø³Ù¹Ù†Ù¹ ÛÛŒÚºÛ” Ø¢Ù¾ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¯Ø±Ø³Øª Ø¬ÙˆØ§Ø¨Ø§Øª Ø¯ÛŒØªÛ’ ÛÛŒÚºÛ”
```

**Translation**: "You are a helpful and harmless AI assistant. You provide correct answers to questions in Urdu."

## ğŸ› ï¸ Model Variants

| Variant | Platform | Size | Quantization |
|---------|----------|------|--------------|
| Qalb-1.0-8B-Instruct | Hugging Face | 8B | BF16 (Full) |
| qalb:8b-instruct-fp16 | Ollama | 16GB | FP16 |
| Quantized versions | Hugging Face | Various | 4-bit, GGUF |

## ğŸ”— Model Tree

```
unsloth/Meta-Llama-3.1-8B (Base Model)
         â”‚
         â”œâ”€â”€ Continued Pre-training (1.97B tokens)
         â”‚
         â”œâ”€â”€ Fine-tuning (Alif Urdu-instruct)
         â”‚
         â””â”€â”€ enstazao/Qalb-1.0-8B-Instruct
                    â”‚
                    â”œâ”€â”€ Finetunes (2 models)
                    â”‚
                    â””â”€â”€ Quantizations (3 models)
```

## âš™ï¸ Inference Parameters

### Recommended Settings

| Parameter | Value |
|-----------|-------|
| **max_new_tokens** | 256 |
| **temperature** | 0.1 |
| **top_p** | 0.9 |
| **repetition_penalty** | 1.1 |
| **do_sample** | True |

### Stop Tokens

```python
eos_token_id = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]
```
