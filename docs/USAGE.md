# Qalb Usage Guide

## ğŸš€ Quick Start

### Option 1: Ollama (Easiest - Recommended for Local Testing)

```bash
# Install Ollama if not already installed
# Download from: https://ollama.com/download

# Pull and run the model
ollama run enstazao/qalb:8b-instruct-fp16
```

### Option 2: Python with Ollama

```python
import ollama

response = ollama.chat(model='enstazao/qalb:8b-instruct-fp16', messages=[
  {
    'role': 'user',
    'content': 'Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ù‚ÙˆÙ…ÛŒ Ú©Ú¾ÛŒÙ„ Ú©ÛŒØ§ ÛÛ’ØŸ',
  },
])

print(response['message']['content'])
```

### Option 3: cURL (REST API)

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "enstazao/qalb:8b-instruct-fp16",
  "prompt": "Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ù‚ÙˆÙ…ÛŒ Ú©Ú¾ÛŒÙ„ Ú©ÛŒØ§ ÛÛ’ØŸ",
  "stream": false
}'
```

## ğŸ Python Usage Methods

### Method 1: Using Unsloth (Recommended - Fast & Efficient)

The easiest and fastest way to run Qalb with 2x faster inference:

```python
from unsloth import FastLanguageModel
import torch

# Load model with 4-bit quantization
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="enstazao/Qalb-1.0-8B-Instruct",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,  # Use 4-bit quantization for memory efficiency
)
FastLanguageModel.for_inference(model)

# System prompt in Urdu
urdu_system_prompt = "Ø¢Ù¾ Ø§ÛŒÚ© Ù…Ø¯Ø¯Ú¯Ø§Ø± Ø§ÙˆØ± Ø¨Û’ Ø¶Ø±Ø± Ù…ØµÙ†ÙˆØ¹ÛŒ Ø°ÛØ§Ù†Øª Ú©Û’ Ø§Ø³Ø³Ù¹Ù†Ù¹ ÛÛŒÚºÛ” Ø¢Ù¾ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¯Ø±Ø³Øª Ø¬ÙˆØ§Ø¨Ø§Øª Ø¯ÛŒØªÛ’ ÛÛŒÚºÛ”"

# Example questions
questions = [
    "Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ù‚ÙˆÙ…ÛŒ Ú©Ú¾ÛŒÙ„ Ú©ÛŒØ§ ÛÛ’ØŸ",
    "Ù„Ø§ÛÙˆØ± Ø´ÛØ± Ú©ÛŒÙˆÚº Ù…Ø´ÛÙˆØ± ÛÛ’ØŸ Ù…Ø®ØªØµØ± ÙˆØ¶Ø§Ø­Øª Ú©Ø±ÛŒÚºÛ”",
    "Ø³ÙˆØ§Ù„: Ù„ÛŒØ§Ù‚Øª Ø¹Ù„ÛŒ Ø®Ø§Ù† Ú©ÙˆÙ† ØªÚ¾Û’ØŸ",
    "Ú©Ø±Ø§Ú†ÛŒ Ú©Ùˆ Ø±ÙˆØ´Ù†ÛŒÙˆÚº Ú©Ø§ Ø´ÛØ± Ú©ÛŒÙˆÚº Ú©ÛØ§ Ø¬Ø§ØªØ§ ÛÛ’ØŸ",
    "Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ù…ÛŒÚº ØªØ±Ø¬Ù…Û Ú©Ø±ÛŒÚº: 'Ù…Ø­Ù†Øª Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ú©ÛŒ Ú©Ù†Ø¬ÛŒ ÛÛ’Û”'"
]

print("ğŸš€ Starting Batch Generation...\n")

for user_input in questions:
    print(f"ğŸ”¹ Question: {user_input}")

    # Format prompt using Llama-3 style
    prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{urdu_system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""

    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.1,
        top_p=0.9,
        repetition_penalty=1.1,
        do_sample=True,
        eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|eot_id|>")]
    )

    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
    
    print(f"âœ… Answer: {response}")
    print("-" * 50)
```

### Method 2: Using Hugging Face Transformers

Compatible with standard transformers if Unsloth is not available:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

model_name = "enstazao/Qalb-1.0-8B-Instruct"
urdu_system_prompt = "Ø¢Ù¾ Ø§ÛŒÚ© Ù…Ø¯Ø¯Ú¯Ø§Ø± Ø§ÙˆØ± Ø¨Û’ Ø¶Ø±Ø± Ù…ØµÙ†ÙˆØ¹ÛŒ Ø°ÛØ§Ù†Øª Ú©Û’ Ø§Ø³Ø³Ù¹Ù†Ù¹ ÛÛŒÚºÛ” Ø¢Ù¾ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¯Ø±Ø³Øª Ø¬ÙˆØ§Ø¨Ø§Øª Ø¯ÛŒØªÛ’ ÛÛŒÚºÛ”"

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

print("â³ Loading model in 4-bit...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

# Example questions
questions = [
    "Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ù‚ÙˆÙ…ÛŒ Ú©Ú¾ÛŒÙ„ Ú©ÛŒØ§ ÛÛ’ØŸ",
    "Ù„Ø§ÛÙˆØ± Ø´ÛØ± Ú©ÛŒÙˆÚº Ù…Ø´ÛÙˆØ± ÛÛ’ØŸ Ù…Ø®ØªØµØ± ÙˆØ¶Ø§Ø­Øª Ú©Ø±ÛŒÚºÛ”",
    "Ø³ÙˆØ§Ù„: Ù„ÛŒØ§Ù‚Øª Ø¹Ù„ÛŒ Ø®Ø§Ù† Ú©ÙˆÙ† ØªÚ¾Û’ØŸ",
    "Ø³ÙˆØ§Ù„: Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯ Ø´ÛØ± Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ø¨ØªØ§Ø¦ÛŒÚºÛ”",
    "Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ù…ÛŒÚº ØªØ±Ø¬Ù…Û Ú©Ø±ÛŒÚº: 'Ù…Ø­Ù†Øª Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ú©ÛŒ Ú©Ù†Ø¬ÛŒ ÛÛ’Û”'"
]

print("Model Loaded. Starting Generation...\n")

for user_input in questions:
    print(f"ğŸ”¹ Question: {user_input}")
    
    prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{urdu_system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""

    input_ids = tokenizer([prompt], return_tensors="pt").to("cuda")

    outputs = model.generate(
        **input_ids,
        max_new_tokens=256,
        temperature=0.1,
        top_p=0.9,
        repetition_penalty=1.1,
        do_sample=True,
        eos_token_id=terminators
    )

    response = tokenizer.decode(outputs[0][input_ids['input_ids'].shape[1]:], skip_special_tokens=True)
    
    print(f"âœ… Answer: {response}")
    print("-" * 50)
```

## ğŸŒ Google Colab

For easy cloud-based testing without local GPU:

[Open In Colab](https://colab.research.google.com/drive/1SQ_OaPhr1Q130FDho89zvughfRxJqdoF?usp=sharing)

## âš™ï¸ Recommended Parameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| `max_new_tokens` | 256 | Maximum tokens to generate |
| `temperature` | 0.1 | Lower = more deterministic |
| `top_p` | 0.9 | Nucleus sampling threshold |
| `repetition_penalty` | 1.1 | Prevents repetitive outputs |
| `do_sample` | True | Enable sampling |

## ğŸ“ Prompt Template

Use the official Llama-3 chat template:

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

### Default System Prompt

**Urdu:**
```
Ø¢Ù¾ Ø§ÛŒÚ© Ù…Ø¯Ø¯Ú¯Ø§Ø± Ø§ÙˆØ± Ø¨Û’ Ø¶Ø±Ø± Ù…ØµÙ†ÙˆØ¹ÛŒ Ø°ÛØ§Ù†Øª Ú©Û’ Ø§Ø³Ø³Ù¹Ù†Ù¹ ÛÛŒÚºÛ” Ø¢Ù¾ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¯Ø±Ø³Øª Ø¬ÙˆØ§Ø¨Ø§Øª Ø¯ÛŒØªÛ’ ÛÛŒÚºÛ”
```

**English Translation:**
```
You are a helpful and harmless AI assistant. You provide correct answers to questions in Urdu.
```

## ğŸ’» System Requirements

### Minimum Requirements

| Component | Requirement |
|-----------|-------------|
| **GPU VRAM** | 8GB (with 4-bit quantization) |
| **RAM** | 16GB |
| **Storage** | 20GB free space |

### Recommended Requirements

| Component | Requirement |
|-----------|-------------|
| **GPU VRAM** | 16GB+ (for FP16) |
| **RAM** | 32GB |
| **Storage** | 40GB free space |

## ğŸ“¦ Installation

### For Unsloth

```bash
pip install unsloth
pip install torch
```

### For Transformers

```bash
pip install transformers
pip install torch
pip install bitsandbytes
pip install accelerate
```

### For Ollama

```bash
# Windows: Download from https://ollama.com/download
# Linux/Mac:
curl -fsSL https://ollama.com/install.sh | sh
```

## ğŸ¯ Example Use Cases

### 1. Question Answering

```python
question = "Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ø¯Ø§Ø±Ø§Ù„Ø­Ú©ÙˆÙ…Øª Ú©ÙˆÙ† Ø³Ø§ Ø´ÛØ± ÛÛ’ØŸ"
# Response: Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ø¯Ø§Ø±Ø§Ù„Ø­Ú©ÙˆÙ…Øª ÛÛ’Û”
```

### 2. Translation (Urdu â†’ English)

```python
question = "Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ù…ÛŒÚº ØªØ±Ø¬Ù…Û Ú©Ø±ÛŒÚº: 'Ù…Ø­Ù†Øª Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ú©ÛŒ Ú©Ù†Ø¬ÛŒ ÛÛ’Û”'"
# Response: Hard work is the key to success.
```

### 3. Creative Writing

```python
question = "Ø§ÛŒÚ© Ù…Ø®ØªØµØ± Ú©ÛØ§Ù†ÛŒ Ù„Ú©Ú¾ÛŒÚº Ø¬Ø³ Ù…ÛŒÚº Ø§ÛŒÚ© Ø¨Ú†Û Ø§ÙˆØ± Ø§Ø³ Ú©Ø§ Ø¯ÙˆØ³Øª ÛÙˆÛ”"
# Response: [Generates creative Urdu story]
```

### 4. Sentiment Analysis

```python
question = "Ø§Ø³ Ø¬Ù…Ù„Û’ Ú©Ø§ Ø¬Ø°Ø¨Ø§Øª Ø¨ØªØ§Ø¦ÛŒÚº: 'Ø¢Ø¬ Ú©Ø§ Ø¯Ù† Ø¨ÛØª Ø§Ú†Ú¾Ø§ Ú¯Ø²Ø±Ø§'"
# Response: ÛŒÛ Ø¬Ù…Ù„Û Ù…Ø«Ø¨Øª Ø¬Ø°Ø¨Ø§Øª Ú©Ø§ Ø§Ø¸ÛØ§Ø± Ú©Ø±ØªØ§ ÛÛ’...
```

### 5. Text Classification

```python
question = "Ø§Ø³ Ø®Ø¨Ø± Ú©ÛŒ Ø¯Ø±Ø¬Û Ø¨Ù†Ø¯ÛŒ Ú©Ø±ÛŒÚº: 'Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù†Û’ Ú©Ø±Ú©Ù¹ Ù…ÛŒÚ† Ø¬ÛŒØª Ù„ÛŒØ§'"
# Response: Ú©Ú¾ÛŒÙ„ (Sports)
```

## âš ï¸ Limitations & Best Practices

### Limitations

- May reflect biases present in training data
- Should not be used as sole source for:
  - Medical advice
  - Legal guidance
  - Religious rulings
- Always fact-check critical information

### Best Practices

1. âœ… Use appropriate system prompts
2. âœ… Set reasonable `max_new_tokens`
3. âœ… Use lower `temperature` for factual tasks
4. âœ… Verify outputs for critical applications
5. âœ… Use 4-bit quantization for memory-constrained systems
